{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "import math\n",
    "import os.path as path\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "\n",
    "# 랜덤 시드 고정 \n",
    "np.random.seed(22)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(284807, 31)\n",
      "               min            max          mean           std          vars  \\\n",
      "Time      0.000000  172792.000000  9.481387e+04  47488.144531  2.255124e+09   \n",
      "V1      -56.407509       2.454930  1.339397e-08      1.958696  3.836489e+00   \n",
      "V2      -72.715729      22.057730  0.000000e+00      1.651309  2.726820e+00   \n",
      "V3      -48.325588       9.382559 -4.800400e-08      1.516255  2.299029e+00   \n",
      "V4       -5.683171      16.875343  6.643411e-09      1.415869  2.004684e+00   \n",
      "V5     -113.743309      34.801666  2.657364e-08      1.380247  1.905081e+00   \n",
      "V6      -26.160505      73.301628 -1.500125e-09      1.332271  1.774946e+00   \n",
      "V7      -43.557243     120.589493 -1.071518e-09      1.237094  1.530401e+00   \n",
      "V8      -73.216721      20.007208 -1.071518e-10      1.194353  1.426479e+00   \n",
      "V9      -13.434067      15.594995 -3.214554e-10      1.098632  1.206993e+00   \n",
      "V10     -24.588263      23.745136 -4.071768e-09      1.088850  1.185594e+00   \n",
      "V11      -4.797473      12.018913  7.714928e-09      1.020713  1.041855e+00   \n",
      "V12     -18.683714       7.848392  8.572143e-09      0.999201  9.984034e-01   \n",
      "V13      -5.791881       7.126883  1.580489e-09      0.995274  9.905708e-01   \n",
      "V14     -19.214325      10.526766 -3.107402e-09      0.958596  9.189056e-01   \n",
      "V15      -4.498945       8.877742 -1.285821e-08      0.915316  8.378034e-01   \n",
      "V16     -14.129854      17.315111  7.500625e-10      0.876253  7.678191e-01   \n",
      "V17     -25.162800       9.253527 -2.625219e-09      0.849337  7.213734e-01   \n",
      "V18      -9.498746       5.041069  2.357339e-09      0.838176  7.025394e-01   \n",
      "V19      -7.213527       5.591971  2.678795e-10      0.814040  6.626619e-01   \n",
      "V20     -54.497719      39.420906 -1.178670e-09      0.770925  5.943254e-01   \n",
      "V21     -34.830383      27.202839 -1.928732e-09      0.734524  5.395255e-01   \n",
      "V22     -10.933144      10.503090  3.643161e-09      0.725702  5.266427e-01   \n",
      "V23     -44.807735      22.528412  3.214554e-10      0.624460  3.899507e-01   \n",
      "V24      -2.836627       4.584549  1.259033e-09      0.605647  3.668084e-01   \n",
      "V25     -10.295397       7.519588  2.143036e-09      0.521278  2.717308e-01   \n",
      "V26      -2.604551       3.517346  1.034684e-09      0.482227  2.325429e-01   \n",
      "V27     -22.565680      31.612198  1.138488e-10      0.403632  1.629192e-01   \n",
      "V28     -15.430084      33.847809 -1.272427e-10      0.330083  1.089550e-01   \n",
      "Amount    0.000000   25691.160156  8.834961e+01    250.120117  6.256007e+04   \n",
      "Class     0.000000       1.000000  1.727486e-03      0.041527  1.724508e-03   \n",
      "\n",
      "              median  minus_cnt  plus_cnt  zero_cnt  \n",
      "Time    84692.000000          0    284805         2  \n",
      "V1          0.018109     141456    143351         0  \n",
      "V2          0.065486     134218    150589         0  \n",
      "V3          0.179846     128163    156644         0  \n",
      "V4         -0.019847     144105    140702         0  \n",
      "V5         -0.054336     148928    135879         0  \n",
      "V6         -0.274187     176633    108174         0  \n",
      "V7          0.040103     135852    148955         0  \n",
      "V8          0.022358     135521    149286         0  \n",
      "V9         -0.051429     149455    135352         0  \n",
      "V10        -0.092917     159922    124885         0  \n",
      "V11        -0.032757     145632    139175         0  \n",
      "V12         0.140033     121089    163718         0  \n",
      "V13        -0.013568     144123    140684         0  \n",
      "V14         0.050601     133762    151045         0  \n",
      "V15         0.048072     136234    148573         0  \n",
      "V16         0.066413     132552    152255         0  \n",
      "V17        -0.065676     153635    131172         0  \n",
      "V18        -0.003636     142942    141865         0  \n",
      "V19         0.003735     141804    143003         0  \n",
      "V20        -0.062481     169130    115677         0  \n",
      "V21        -0.029450     153196    131611         0  \n",
      "V22         0.006782     141402    143405         0  \n",
      "V23        -0.011193     148400    136407         0  \n",
      "V24         0.040976     127732    157075         0  \n",
      "V25         0.016594     139302    145505         0  \n",
      "V26        -0.052139     151720    133087         0  \n",
      "V27         0.001342     141004    143803         0  \n",
      "V28         0.011244     125888    158919         0  \n",
      "Amount     22.000000          0    282982      1825  \n",
      "Class       0.000000          0       492    284315  \n"
     ]
    }
   ],
   "source": [
    "# 데이터 분포 확인하기 \n",
    "# csv 파일 가져오기 \n",
    "df = pd.read_csv(\"dataset/creditcard.csv\", delimiter=',', dtype=np.float32)\n",
    "\n",
    "# row 284807, col 31\n",
    "print(df.shape)\n",
    "\n",
    "distribution = pd.DataFrame( {\"min\": [ df[col].min() for col in df.columns],   # 최소값\n",
    "                            \"max\": [ df[col].max() for col in df.columns],     # 최대값\n",
    "                            \"mean\": [ df[col].mean() for col in df.columns],   # 평균값\n",
    "                            \"std\": [ df[col].std() for col in df.columns],     # 표준 편차\n",
    "                            \"vars\": [ df[col].var() for col in df.columns],    # 분산\n",
    "                            \"median\": [ df[col].median() for col in df.columns],    # 중앙값\n",
    "                            #\"mode\": [ df[col].mode(dropna=False) for col in df.columns],    # 최빈값\n",
    "                            \"minus_cnt\" : [ len(list(filter(lambda x : x < 0 , df[col]))) for col in df.columns], # 음수 개수\n",
    "                            \"plus_cnt\" : [ len(list(filter(lambda x : x > 0. , df[col]))) for col in df.columns], # 양수 개수\n",
    "                            \"zero_cnt\" : [ len(list(filter(lambda x : x == 0. , df[col]))) for col in df.columns] # 0 개수\n",
    "                            },\n",
    "    index=[df.columns],\n",
    ")\n",
    "print(distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전처리 유틸 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creditcard.csv 를 training set / test set (혹은 training set/ validation set/ test set ) 으로 분리하는 전처리 클래스\n",
    "# 랜덤으로 나눠주는 유틸함수는 있는데, 나눌때 이상거래 비율을 맞춰서 나누는 유틸 함수는 찾지못해서 직접 구현 \n",
    "# 이런 기능이 tensorflow에 있는지 모르겠음\n",
    "class Preprocessing:\n",
    "\n",
    "    def __init__(self, early_stopping=False):\n",
    "        # 여러 실험해서 좋은 점수를 내는 모델을 찾아야 하므로  \n",
    "        # training set / validation set / test set등은 딱한번 구해서 파일로 저장하고 고정해서 사용 \n",
    "        if not path.isfile(\"dataset/train_set.csv\") :\n",
    "            self.save_dividing_sample_csv(srcpath=\"dataset/creditcard.csv\", \n",
    "                                          trainpath=\"dataset/train_set.csv\",\n",
    "                                          samplepath=\"dataset/test_set.csv\",\n",
    "                                          sample_ratio=0.3)\n",
    "                    \n",
    "        if early_stopping and not path.isfile(\"dataset/early_stopping/train_set.csv\") :\n",
    "                \n",
    "            self.save_dividing_sample_csv(srcpath=\"dataset/train_set.csv\", \n",
    "                                          trainpath=\"dataset/early_stopping/train_set.csv\",\n",
    "                                          samplepath=\"dataset/early_stopping/validation_set.csv\",\n",
    "                                          sample_ratio=0.3)\n",
    "            \n",
    "            shutil.copyfile(\"dataset/test_set.csv\", \"dataset/early_stopping/test_set.csv\")\n",
    "\n",
    "\n",
    "    def save_dividing_sample_csv(self, srcpath, trainpath, samplepath, sample_ratio):\n",
    "        # csv 파일 가져오기 \n",
    "        df = pd.read_csv(srcpath, delimiter=',', dtype=np.float32)\n",
    "        if \"Time\" in df.columns:\n",
    "            df.drop(columns = \"Time\", inplace=True)\n",
    "        \n",
    "        train_data, sample_data = self.divide_sample(df, sample_ratio)\n",
    "        \n",
    "        train_data.to_csv(trainpath, sep=\",\", encoding='utf-8')\n",
    "        sample_data.to_csv(samplepath, sep=\",\", encoding='utf-8')\n",
    "        \n",
    "    def divide_sample(self, df, dividing_ratio):\n",
    "        #전체 데이터에서 sample과 training set 나눠서 리턴 \n",
    "        \n",
    "        # 훈련데이터 / 테스트 데이터는 동일 분포를 가지는게 좋다고함\n",
    "        # 동일 분포를 가지게 하는거는 잘 모르겠고 이상거래 비율 정도만 맞춰서 샘플링 \n",
    "        sample_len = math.ceil(len(df.values) * dividing_ratio) # dividing_ratio 따른 샘플 개수 구하기 \n",
    "        fraud_ratio = df[\"Class\"].value_counts()[1] / df[\"Class\"].value_counts()[0] # 정상대비 이상거래 비율 계산 \n",
    "        fraud_len = round(sample_len * fraud_ratio) # 샘플 개수 / 이상거래 비율 따른 이상거래 개수 구하기  \n",
    "    \n",
    "        fraud = df.loc[df[\"Class\"] == 1]\n",
    "        normal = df.loc[df[\"Class\"] == 0]\n",
    "    \n",
    "        # 현재 이상거래 비율(0.00017) 만큼 인덱스 랜덤 선택/ 나머지는 정상거래 인덱스 랜덤 선택 \n",
    "        sample_fraud_idx = np.random.choice(fraud.index.tolist(), size=fraud_len, replace=False, p=None)\n",
    "        sample_normal_idx = np.random.choice(normal.index.tolist(), size=sample_len-fraud_len, replace=False, p=None)\n",
    "        sample_idx = shuffle(np.concatenate((sample_fraud_idx, sample_normal_idx))) # 추출 인덱스 섞기 \n",
    "    \n",
    "        # 인덱스로부터 dataframe 구하기\n",
    "        sample_data = pd.concat([df.iloc[[idx]] for idx in sample_idx]).reset_index()\n",
    "        train_data = pd.concat([df.iloc[[idx]] for idx in df.index.tolist() if idx not in sample_idx]).reset_index()\n",
    "    \n",
    "        return train_data, sample_data \n",
    "\n",
    "\n",
    "    def load_data(self, fpath):\n",
    "        #file path csv 읽어서 output 데이터 , input 데이터 나눠서 리턴\n",
    "    \n",
    "        data = pd.read_csv(fpath, delimiter=',', dtype=np.float32)\n",
    "        \n",
    "        x_data = np.array(data.iloc[:,0:-1].values, dtype=np.float32) # input 컬럼들 추출\n",
    "        y_data = np.array(data.iloc[:,[-1]].values, dtype=np.float32) # output(class) 컬럼 추출\n",
    "    \n",
    "        return x_data, y_data\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기본 \n",
    "Normalization + Dropout 만 추가하여 테스트 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "49/49 [==============================] - 2s 12ms/step - loss: 0.0558 - f1_score: 0.0034 - recall_9: 0.0203\n",
      "Epoch 2/100\n",
      "49/49 [==============================] - 1s 12ms/step - loss: 0.0169 - f1_score: 0.0034 - recall_9: 0.0000e+00\n",
      "Epoch 3/100\n",
      "49/49 [==============================] - 1s 11ms/step - loss: 0.0144 - f1_score: 0.0034 - recall_9: 0.0000e+00\n",
      "Epoch 4/100\n",
      "49/49 [==============================] - 1s 12ms/step - loss: 0.0123 - f1_score: 0.0034 - recall_9: 0.0000e+00\n",
      "Epoch 5/100\n",
      "49/49 [==============================] - 1s 12ms/step - loss: 0.0106 - f1_score: 0.0034 - recall_9: 0.0000e+00\n",
      "Epoch 6/100\n",
      "49/49 [==============================] - 1s 11ms/step - loss: 0.0091 - f1_score: 0.0034 - recall_9: 0.0029\n",
      "Epoch 7/100\n",
      "49/49 [==============================] - 1s 12ms/step - loss: 0.0080 - f1_score: 0.0034 - recall_9: 0.0320\n",
      "Epoch 8/100\n",
      "49/49 [==============================] - 1s 11ms/step - loss: 0.0074 - f1_score: 0.0034 - recall_9: 0.2064\n",
      "Epoch 9/100\n",
      "49/49 [==============================] - 1s 12ms/step - loss: 0.0068 - f1_score: 0.0034 - recall_9: 0.3634\n",
      "Epoch 10/100\n",
      "49/49 [==============================] - 1s 12ms/step - loss: 0.0065 - f1_score: 0.0034 - recall_9: 0.4419\n",
      "Epoch 11/100\n",
      "49/49 [==============================] - 1s 14ms/step - loss: 0.0064 - f1_score: 0.0034 - recall_9: 0.4419\n",
      "Epoch 12/100\n",
      "49/49 [==============================] - 1s 15ms/step - loss: 0.0061 - f1_score: 0.0034 - recall_9: 0.4767\n",
      "Epoch 13/100\n",
      "49/49 [==============================] - 1s 12ms/step - loss: 0.0058 - f1_score: 0.0034 - recall_9: 0.5465\n",
      "Epoch 14/100\n",
      "49/49 [==============================] - 1s 11ms/step - loss: 0.0056 - f1_score: 0.0034 - recall_9: 0.5465\n",
      "Epoch 15/100\n",
      "49/49 [==============================] - 1s 10ms/step - loss: 0.0053 - f1_score: 0.0034 - recall_9: 0.4971\n",
      "Epoch 16/100\n",
      "49/49 [==============================] - 1s 12ms/step - loss: 0.0054 - f1_score: 0.0034 - recall_9: 0.5320\n",
      "Epoch 17/100\n",
      "49/49 [==============================] - 1s 11ms/step - loss: 0.0057 - f1_score: 0.0034 - recall_9: 0.5233\n",
      "Epoch 18/100\n",
      "49/49 [==============================] - 1s 10ms/step - loss: 0.0056 - f1_score: 0.0034 - recall_9: 0.5174\n",
      "Epoch 19/100\n",
      "49/49 [==============================] - 1s 11ms/step - loss: 0.0053 - f1_score: 0.0034 - recall_9: 0.5669\n",
      "Epoch 20/100\n",
      "49/49 [==============================] - 1s 10ms/step - loss: 0.0058 - f1_score: 0.0034 - recall_9: 0.4942\n",
      "Epoch 21/100\n",
      "49/49 [==============================] - 1s 11ms/step - loss: 0.0057 - f1_score: 0.0034 - recall_9: 0.5233\n",
      "Epoch 22/100\n",
      "49/49 [==============================] - 1s 11ms/step - loss: 0.0056 - f1_score: 0.0034 - recall_9: 0.5262\n",
      "Epoch 23/100\n",
      "49/49 [==============================] - 1s 10ms/step - loss: 0.0056 - f1_score: 0.0034 - recall_9: 0.5233\n",
      "Epoch 24/100\n",
      "49/49 [==============================] - 1s 11ms/step - loss: 0.0058 - f1_score: 0.0034 - recall_9: 0.5640\n",
      "Epoch 25/100\n",
      "49/49 [==============================] - 1s 11ms/step - loss: 0.0059 - f1_score: 0.0035 - recall_9: 0.5552\n",
      "Epoch 26/100\n",
      "49/49 [==============================] - 1s 10ms/step - loss: 0.0059 - f1_score: 0.0034 - recall_9: 0.5610\n",
      "Epoch 27/100\n",
      "49/49 [==============================] - 1s 11ms/step - loss: 0.0054 - f1_score: 0.0034 - recall_9: 0.5523\n",
      "Epoch 28/100\n",
      "49/49 [==============================] - 1s 11ms/step - loss: 0.0057 - f1_score: 0.0034 - recall_9: 0.5843\n",
      "Epoch 29/100\n",
      "49/49 [==============================] - 1s 10ms/step - loss: 0.0057 - f1_score: 0.0034 - recall_9: 0.5436\n",
      "Epoch 30/100\n",
      "49/49 [==============================] - 1s 10ms/step - loss: 0.0061 - f1_score: 0.0034 - recall_9: 0.5320\n",
      "Epoch 31/100\n",
      "49/49 [==============================] - 1s 10ms/step - loss: 0.0053 - f1_score: 0.0034 - recall_9: 0.5901\n",
      "Epoch 32/100\n",
      "49/49 [==============================] - 1s 11ms/step - loss: 0.0055 - f1_score: 0.0034 - recall_9: 0.5785\n",
      "Epoch 33/100\n",
      "49/49 [==============================] - 1s 11ms/step - loss: 0.0058 - f1_score: 0.0035 - recall_9: 0.5378\n",
      "Epoch 34/100\n",
      "49/49 [==============================] - 1s 11ms/step - loss: 0.0054 - f1_score: 0.0035 - recall_9: 0.5640\n",
      "Epoch 35/100\n",
      "49/49 [==============================] - 1s 11ms/step - loss: 0.0051 - f1_score: 0.0034 - recall_9: 0.5814\n",
      "Epoch 36/100\n",
      "49/49 [==============================] - 1s 11ms/step - loss: 0.0047 - f1_score: 0.0034 - recall_9: 0.6192\n",
      "Epoch 37/100\n",
      "49/49 [==============================] - 1s 10ms/step - loss: 0.0047 - f1_score: 0.0034 - recall_9: 0.6831\n",
      "Epoch 38/100\n",
      "49/49 [==============================] - 1s 11ms/step - loss: 0.0047 - f1_score: 0.0034 - recall_9: 0.6395\n",
      "Epoch 39/100\n",
      "49/49 [==============================] - 1s 11ms/step - loss: 0.0050 - f1_score: 0.0034 - recall_9: 0.6599\n",
      "Epoch 40/100\n",
      "49/49 [==============================] - 1s 11ms/step - loss: 0.0050 - f1_score: 0.0034 - recall_9: 0.6453\n",
      "Epoch 41/100\n",
      "49/49 [==============================] - 1s 11ms/step - loss: 0.0047 - f1_score: 0.0034 - recall_9: 0.6744\n",
      "Epoch 42/100\n",
      "49/49 [==============================] - 1s 10ms/step - loss: 0.0050 - f1_score: 0.0034 - recall_9: 0.6105\n",
      "Epoch 43/100\n",
      "49/49 [==============================] - 1s 10ms/step - loss: 0.0051 - f1_score: 0.0034 - recall_9: 0.6163\n",
      "Epoch 44/100\n",
      "49/49 [==============================] - 1s 11ms/step - loss: 0.0050 - f1_score: 0.0035 - recall_9: 0.6483\n",
      "Epoch 45/100\n",
      "49/49 [==============================] - 1s 11ms/step - loss: 0.0050 - f1_score: 0.0035 - recall_9: 0.6017\n",
      "Epoch 46/100\n",
      "49/49 [==============================] - 1s 11ms/step - loss: 0.0049 - f1_score: 0.0035 - recall_9: 0.6570\n",
      "Epoch 47/100\n",
      "49/49 [==============================] - 1s 11ms/step - loss: 0.0050 - f1_score: 0.0035 - recall_9: 0.6134\n",
      "Epoch 48/100\n",
      "49/49 [==============================] - 1s 11ms/step - loss: 0.0050 - f1_score: 0.0034 - recall_9: 0.6279\n",
      "Epoch 49/100\n",
      "49/49 [==============================] - 1s 11ms/step - loss: 0.0050 - f1_score: 0.0035 - recall_9: 0.6076\n",
      "Epoch 50/100\n",
      "49/49 [==============================] - 1s 13ms/step - loss: 0.0048 - f1_score: 0.0034 - recall_9: 0.6337\n",
      "Epoch 51/100\n",
      "49/49 [==============================] - 1s 12ms/step - loss: 0.0048 - f1_score: 0.0035 - recall_9: 0.6366\n",
      "Epoch 52/100\n",
      "49/49 [==============================] - 1s 13ms/step - loss: 0.0050 - f1_score: 0.0035 - recall_9: 0.6047\n",
      "Epoch 53/100\n",
      "49/49 [==============================] - 1s 13ms/step - loss: 0.0051 - f1_score: 0.0035 - recall_9: 0.5901\n",
      "Epoch 54/100\n",
      "49/49 [==============================] - 1s 12ms/step - loss: 0.0047 - f1_score: 0.0034 - recall_9: 0.5843\n",
      "Epoch 55/100\n",
      "49/49 [==============================] - 1s 11ms/step - loss: 0.0050 - f1_score: 0.0035 - recall_9: 0.6076\n",
      "Epoch 56/100\n",
      "49/49 [==============================] - 1s 11ms/step - loss: 0.0050 - f1_score: 0.0035 - recall_9: 0.5669\n",
      "Epoch 57/100\n",
      "49/49 [==============================] - 1s 11ms/step - loss: 0.0050 - f1_score: 0.0035 - recall_9: 0.5523\n",
      "Epoch 58/100\n",
      "49/49 [==============================] - 1s 11ms/step - loss: 0.0053 - f1_score: 0.0035 - recall_9: 0.5872\n",
      "Epoch 59/100\n",
      "49/49 [==============================] - 1s 11ms/step - loss: 0.0049 - f1_score: 0.0035 - recall_9: 0.5843\n",
      "Epoch 60/100\n",
      "49/49 [==============================] - 1s 11ms/step - loss: 0.0047 - f1_score: 0.0034 - recall_9: 0.6453\n",
      "Epoch 61/100\n",
      "49/49 [==============================] - 1s 11ms/step - loss: 0.0048 - f1_score: 0.0035 - recall_9: 0.6424\n",
      "Epoch 62/100\n",
      "49/49 [==============================] - 1s 11ms/step - loss: 0.0046 - f1_score: 0.0035 - recall_9: 0.5872\n",
      "Epoch 63/100\n",
      "49/49 [==============================] - 1s 12ms/step - loss: 0.0047 - f1_score: 0.0035 - recall_9: 0.5785\n",
      "Epoch 64/100\n",
      "49/49 [==============================] - 1s 13ms/step - loss: 0.0048 - f1_score: 0.0035 - recall_9: 0.5959\n",
      "Epoch 65/100\n",
      "49/49 [==============================] - 1s 12ms/step - loss: 0.0047 - f1_score: 0.0035 - recall_9: 0.6221\n",
      "Epoch 66/100\n",
      "49/49 [==============================] - 1s 12ms/step - loss: 0.0046 - f1_score: 0.0035 - recall_9: 0.6134\n",
      "Epoch 67/100\n",
      "49/49 [==============================] - 1s 14ms/step - loss: 0.0046 - f1_score: 0.0035 - recall_9: 0.5988\n",
      "Epoch 68/100\n",
      "49/49 [==============================] - 1s 12ms/step - loss: 0.0046 - f1_score: 0.0035 - recall_9: 0.6453\n",
      "Epoch 69/100\n",
      "49/49 [==============================] - 1s 12ms/step - loss: 0.0046 - f1_score: 0.0035 - recall_9: 0.6076\n",
      "Epoch 70/100\n",
      "49/49 [==============================] - 1s 12ms/step - loss: 0.0048 - f1_score: 0.0035 - recall_9: 0.5785\n",
      "Epoch 71/100\n",
      "49/49 [==============================] - 1s 13ms/step - loss: 0.0045 - f1_score: 0.0035 - recall_9: 0.6337\n",
      "Epoch 72/100\n",
      "49/49 [==============================] - 1s 13ms/step - loss: 0.0047 - f1_score: 0.0035 - recall_9: 0.5930\n",
      "Epoch 73/100\n",
      "49/49 [==============================] - 1s 13ms/step - loss: 0.0045 - f1_score: 0.0035 - recall_9: 0.6628\n",
      "Epoch 74/100\n",
      "49/49 [==============================] - 1s 13ms/step - loss: 0.0046 - f1_score: 0.0035 - recall_9: 0.6192\n",
      "Epoch 75/100\n",
      "49/49 [==============================] - 1s 12ms/step - loss: 0.0049 - f1_score: 0.0035 - recall_9: 0.5930\n",
      "Epoch 76/100\n",
      "49/49 [==============================] - 1s 13ms/step - loss: 0.0043 - f1_score: 0.0035 - recall_9: 0.6686\n",
      "Epoch 77/100\n",
      "49/49 [==============================] - 1s 13ms/step - loss: 0.0046 - f1_score: 0.0035 - recall_9: 0.6250\n",
      "Epoch 78/100\n",
      "49/49 [==============================] - 1s 12ms/step - loss: 0.0043 - f1_score: 0.0035 - recall_9: 0.6192\n",
      "Epoch 79/100\n",
      "49/49 [==============================] - 1s 13ms/step - loss: 0.0047 - f1_score: 0.0035 - recall_9: 0.6134\n",
      "Epoch 80/100\n",
      "49/49 [==============================] - 1s 13ms/step - loss: 0.0044 - f1_score: 0.0035 - recall_9: 0.6570\n",
      "Epoch 81/100\n",
      "49/49 [==============================] - 1s 13ms/step - loss: 0.0043 - f1_score: 0.0035 - recall_9: 0.6308\n",
      "Epoch 82/100\n",
      "49/49 [==============================] - 1s 13ms/step - loss: 0.0046 - f1_score: 0.0035 - recall_9: 0.6424\n",
      "Epoch 83/100\n",
      "49/49 [==============================] - 1s 14ms/step - loss: 0.0044 - f1_score: 0.0035 - recall_9: 0.6512\n",
      "Epoch 84/100\n",
      "49/49 [==============================] - 1s 13ms/step - loss: 0.0041 - f1_score: 0.0035 - recall_9: 0.6192\n",
      "Epoch 85/100\n",
      "49/49 [==============================] - 1s 13ms/step - loss: 0.0045 - f1_score: 0.0035 - recall_9: 0.6076\n",
      "Epoch 86/100\n",
      "49/49 [==============================] - 1s 13ms/step - loss: 0.0043 - f1_score: 0.0035 - recall_9: 0.6395\n",
      "Epoch 87/100\n",
      "49/49 [==============================] - 1s 13ms/step - loss: 0.0043 - f1_score: 0.0035 - recall_9: 0.6308\n",
      "Epoch 88/100\n",
      "49/49 [==============================] - 1s 13ms/step - loss: 0.0044 - f1_score: 0.0035 - recall_9: 0.5930\n",
      "Epoch 89/100\n",
      "49/49 [==============================] - 1s 14ms/step - loss: 0.0045 - f1_score: 0.0035 - recall_9: 0.6047\n",
      "Epoch 90/100\n",
      "49/49 [==============================] - 1s 13ms/step - loss: 0.0043 - f1_score: 0.0035 - recall_9: 0.6134\n",
      "Epoch 91/100\n",
      "49/49 [==============================] - 1s 13ms/step - loss: 0.0046 - f1_score: 0.0035 - recall_9: 0.6250\n",
      "Epoch 92/100\n",
      "49/49 [==============================] - 1s 12ms/step - loss: 0.0043 - f1_score: 0.0035 - recall_9: 0.6424\n",
      "Epoch 93/100\n",
      "49/49 [==============================] - 1s 13ms/step - loss: 0.0041 - f1_score: 0.0035 - recall_9: 0.6483\n",
      "Epoch 94/100\n",
      "49/49 [==============================] - 1s 13ms/step - loss: 0.0043 - f1_score: 0.0035 - recall_9: 0.6250\n",
      "Epoch 95/100\n",
      "49/49 [==============================] - 1s 13ms/step - loss: 0.0044 - f1_score: 0.0035 - recall_9: 0.5930\n",
      "Epoch 96/100\n",
      "49/49 [==============================] - 1s 12ms/step - loss: 0.0043 - f1_score: 0.0035 - recall_9: 0.5872\n",
      "Epoch 97/100\n",
      "49/49 [==============================] - 1s 12ms/step - loss: 0.0042 - f1_score: 0.0035 - recall_9: 0.6279\n",
      "Epoch 98/100\n",
      "49/49 [==============================] - 1s 12ms/step - loss: 0.0045 - f1_score: 0.0034 - recall_9: 0.6047\n",
      "Epoch 99/100\n",
      "49/49 [==============================] - 1s 12ms/step - loss: 0.0042 - f1_score: 0.0035 - recall_9: 0.6076\n",
      "Epoch 100/100\n",
      "49/49 [==============================] - 1s 13ms/step - loss: 0.0043 - f1_score: 0.0035 - recall_9: 0.6453\n"
     ]
    }
   ],
   "source": [
    "pr = Preprocessing()\n",
    "x_train, y_train = pr.load_data(\"dataset/train_set.csv\")\n",
    "x_test, y_test = pr.load_data(\"dataset/test_set.csv\")\n",
    "\n",
    "# Normalization\n",
    "scaler = MinMaxScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.fit_transform(x_test) # test set도 normalization해야하는지?? \n",
    "\n",
    "# 기본 \n",
    "# Model Build\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(29, activation = 'relu'), # 입력층\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(15, activation = 'relu'), # 은닉층 (필요한가??)\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')  # 출력층\n",
    "])\n",
    "\n",
    "# Hyper-parameter tunning (optmizer, cost function)\n",
    "learning_rate = 0.008\n",
    "batch_size = 4096 # 2^12, 그래픽카드가 없어 gpu 활용은 힘듬. 대신 batch size 세팅을 했더니 속도가 많이 개선됨\n",
    "epoch= 100\n",
    "\n",
    "\n",
    "# ndarray -> dataset 변환 후 batch size세팅\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(batch_size) # buffer size?? \n",
    "\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "              loss = 'binary_crossentropy',\n",
    "              metrics=[ tf.keras.metrics.F1Score(), tf.keras.metrics.Recall()]) # 비대칭 데이터 이고 이상거래 탐지가 핵심이므로  recall 수치 필요 \n",
    "\n",
    "# Training\n",
    "try:\n",
    "    history = model.fit(dataset, epochs=epoch)\n",
    "except Exception as e: \n",
    "    print(e)\n",
    "    raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2671/2671 - 4s - loss: 0.0031 - f1_score: 0.0035 - recall_9: 0.8243 - 4s/epoch - 1ms/step\n",
      "f1_score: [0.00345835], recall: 0.8243243098258972\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Visualization\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.xlabel(\"The number of learning\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Evaluation\n",
    "loss, f1_score, recall = model.evaluate(x_test, y_test, verbose=2)\n",
    "print(f\"f1_score: {f1_score}, recall: {recall}\")\n",
    "\n",
    "\n",
    "#print(\"Accuracy = \", accuracy_score(predicted_x, y_data))\n",
    "#print(\"Report = \\n\", classification_report(predicted_x, y_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "recall이 0.8, f1 score가 0.0030"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 개선버전 1: Early Stopping\n",
    "배운 것  모두 다 넣어서 테스트 \n",
    "- dropout\n",
    "- standard scaler\n",
    "- batch normalization\n",
    "- weight initialization \n",
    "- early stopping with patience\n",
    "- learning rate decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "35/35 [==============================] - 5s 94ms/step - loss: 0.0743 - f1_score: 0.0034 - recall_8: 0.4066 - val_loss: 0.0895 - val_f1_score: 0.0035 - val_recall_8: 0.8641\n",
      "Epoch 2/1000\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 0.0093 - f1_score: 0.0034 - recall_8: 0.4813 - val_loss: 0.0348 - val_f1_score: 0.0034 - val_recall_8: 0.8641\n",
      "Epoch 3/1000\n",
      "35/35 [==============================] - 3s 91ms/step - loss: 0.0061 - f1_score: 0.0034 - recall_8: 0.5602 - val_loss: 0.0126 - val_f1_score: 0.0034 - val_recall_8: 0.8350\n",
      "Epoch 4/1000\n",
      "35/35 [==============================] - 3s 84ms/step - loss: 0.0052 - f1_score: 0.0034 - recall_8: 0.5602 - val_loss: 0.0087 - val_f1_score: 0.0034 - val_recall_8: 0.8252\n",
      "Epoch 5/1000\n",
      "35/35 [==============================] - 3s 86ms/step - loss: 0.0050 - f1_score: 0.0034 - recall_8: 0.5768 - val_loss: 0.0085 - val_f1_score: 0.0034 - val_recall_8: 0.8252\n",
      "Epoch 6/1000\n",
      "35/35 [==============================] - 3s 85ms/step - loss: 0.0046 - f1_score: 0.0034 - recall_8: 0.5851 - val_loss: 0.0083 - val_f1_score: 0.0034 - val_recall_8: 0.8155\n",
      "Epoch 7/1000\n",
      "35/35 [==============================] - 3s 98ms/step - loss: 0.0045 - f1_score: 0.0034 - recall_8: 0.6017 - val_loss: 0.0073 - val_f1_score: 0.0034 - val_recall_8: 0.8155\n",
      "Epoch 8/1000\n",
      "35/35 [==============================] - 3s 95ms/step - loss: 0.0046 - f1_score: 0.0034 - recall_8: 0.6141 - val_loss: 0.0063 - val_f1_score: 0.0034 - val_recall_8: 0.8155\n",
      "Epoch 9/1000\n",
      "35/35 [==============================] - 3s 93ms/step - loss: 0.0045 - f1_score: 0.0034 - recall_8: 0.6100 - val_loss: 0.0052 - val_f1_score: 0.0034 - val_recall_8: 0.7767\n",
      "Epoch 10/1000\n",
      "35/35 [==============================] - 3s 94ms/step - loss: 0.0039 - f1_score: 0.0034 - recall_8: 0.6224 - val_loss: 0.0051 - val_f1_score: 0.0034 - val_recall_8: 0.7379\n",
      "Epoch 11/1000\n",
      "35/35 [==============================] - 3s 94ms/step - loss: 0.0040 - f1_score: 0.0034 - recall_8: 0.6224 - val_loss: 0.0049 - val_f1_score: 0.0034 - val_recall_8: 0.6893\n",
      "Epoch 12/1000\n",
      "35/35 [==============================] - 3s 96ms/step - loss: 0.0038 - f1_score: 0.0034 - recall_8: 0.6307 - val_loss: 0.0046 - val_f1_score: 0.0034 - val_recall_8: 0.6214\n",
      "Epoch 13/1000\n",
      "35/35 [==============================] - 3s 86ms/step - loss: 0.0039 - f1_score: 0.0034 - recall_8: 0.6058 - val_loss: 0.0047 - val_f1_score: 0.0034 - val_recall_8: 0.6990\n",
      "Epoch 14/1000\n",
      "35/35 [==============================] - 3s 88ms/step - loss: 0.0040 - f1_score: 0.0034 - recall_8: 0.5851 - val_loss: 0.0051 - val_f1_score: 0.0034 - val_recall_8: 0.7864\n",
      "Epoch 15/1000\n",
      "35/35 [==============================] - 3s 84ms/step - loss: 0.0040 - f1_score: 0.0035 - recall_8: 0.6307 - val_loss: 0.0047 - val_f1_score: 0.0034 - val_recall_8: 0.7573\n",
      "Epoch 16/1000\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 0.0037 - f1_score: 0.0035 - recall_8: 0.6473 - val_loss: 0.0047 - val_f1_score: 0.0034 - val_recall_8: 0.7282\n",
      "Epoch 17/1000\n",
      "35/35 [==============================] - 3s 85ms/step - loss: 0.0033 - f1_score: 0.0035 - recall_8: 0.6763 - val_loss: 0.0046 - val_f1_score: 0.0034 - val_recall_8: 0.7184\n",
      "Epoch 18/1000\n",
      "35/35 [==============================] - 3s 83ms/step - loss: 0.0037 - f1_score: 0.0035 - recall_8: 0.6473 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6602\n",
      "Epoch 19/1000\n",
      "35/35 [==============================] - 3s 85ms/step - loss: 0.0034 - f1_score: 0.0035 - recall_8: 0.6390 - val_loss: 0.0045 - val_f1_score: 0.0034 - val_recall_8: 0.6602\n",
      "Epoch 20/1000\n",
      "35/35 [==============================] - 3s 90ms/step - loss: 0.0033 - f1_score: 0.0035 - recall_8: 0.6598 - val_loss: 0.0046 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 21/1000\n",
      "35/35 [==============================] - 3s 96ms/step - loss: 0.0034 - f1_score: 0.0035 - recall_8: 0.6349 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.5922\n",
      "Epoch 22/1000\n",
      "35/35 [==============================] - 4s 103ms/step - loss: 0.0034 - f1_score: 0.0035 - recall_8: 0.6515 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6602\n",
      "Epoch 23/1000\n",
      "35/35 [==============================] - 4s 105ms/step - loss: 0.0033 - f1_score: 0.0035 - recall_8: 0.6680 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6408\n",
      "Epoch 24/1000\n",
      "35/35 [==============================] - 3s 97ms/step - loss: 0.0031 - f1_score: 0.0035 - recall_8: 0.6598 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6602\n",
      "Epoch 25/1000\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 0.0031 - f1_score: 0.0035 - recall_8: 0.6805 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6505\n",
      "Epoch 26/1000\n",
      "35/35 [==============================] - 3s 91ms/step - loss: 0.0033 - f1_score: 0.0035 - recall_8: 0.6473 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6505\n",
      "Epoch 27/1000\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 0.0033 - f1_score: 0.0035 - recall_8: 0.6598 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6602\n",
      "Epoch 28/1000\n",
      "35/35 [==============================] - 3s 86ms/step - loss: 0.0031 - f1_score: 0.0035 - recall_8: 0.6556 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6602\n",
      "Epoch 29/1000\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 0.0033 - f1_score: 0.0035 - recall_8: 0.6515 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6602\n",
      "Epoch 30/1000\n",
      "35/35 [==============================] - 3s 89ms/step - loss: 0.0031 - f1_score: 0.0035 - recall_8: 0.6722 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6602\n",
      "Epoch 31/1000\n",
      "35/35 [==============================] - 3s 89ms/step - loss: 0.0032 - f1_score: 0.0035 - recall_8: 0.6556 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 32/1000\n",
      "35/35 [==============================] - 3s 88ms/step - loss: 0.0033 - f1_score: 0.0035 - recall_8: 0.6722 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 33/1000\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 0.0032 - f1_score: 0.0035 - recall_8: 0.6598 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6602\n",
      "Epoch 34/1000\n",
      "35/35 [==============================] - 3s 95ms/step - loss: 0.0033 - f1_score: 0.0035 - recall_8: 0.6473 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 35/1000\n",
      "35/35 [==============================] - 3s 96ms/step - loss: 0.0030 - f1_score: 0.0035 - recall_8: 0.6639 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 36/1000\n",
      "35/35 [==============================] - 3s 88ms/step - loss: 0.0034 - f1_score: 0.0035 - recall_8: 0.6224 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 37/1000\n",
      "35/35 [==============================] - 3s 99ms/step - loss: 0.0033 - f1_score: 0.0035 - recall_8: 0.6224 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 38/1000\n",
      "35/35 [==============================] - 3s 89ms/step - loss: 0.0033 - f1_score: 0.0035 - recall_8: 0.6515 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 39/1000\n",
      "35/35 [==============================] - 3s 91ms/step - loss: 0.0034 - f1_score: 0.0035 - recall_8: 0.6473 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 40/1000\n",
      "35/35 [==============================] - 3s 98ms/step - loss: 0.0032 - f1_score: 0.0035 - recall_8: 0.6639 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 41/1000\n",
      "35/35 [==============================] - 3s 96ms/step - loss: 0.0031 - f1_score: 0.0035 - recall_8: 0.6598 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 42/1000\n",
      "35/35 [==============================] - 3s 90ms/step - loss: 0.0033 - f1_score: 0.0035 - recall_8: 0.6598 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 43/1000\n",
      "35/35 [==============================] - 3s 98ms/step - loss: 0.0032 - f1_score: 0.0035 - recall_8: 0.6929 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 44/1000\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 0.0035 - f1_score: 0.0035 - recall_8: 0.6349 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 45/1000\n",
      "35/35 [==============================] - 3s 92ms/step - loss: 0.0034 - f1_score: 0.0035 - recall_8: 0.6390 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 46/1000\n",
      "35/35 [==============================] - 3s 85ms/step - loss: 0.0034 - f1_score: 0.0035 - recall_8: 0.6100 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 47/1000\n",
      "35/35 [==============================] - 4s 111ms/step - loss: 0.0031 - f1_score: 0.0035 - recall_8: 0.6929 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 48/1000\n",
      "35/35 [==============================] - 18s 516ms/step - loss: 0.0033 - f1_score: 0.0035 - recall_8: 0.6307 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 49/1000\n",
      "35/35 [==============================] - 13s 379ms/step - loss: 0.0030 - f1_score: 0.0035 - recall_8: 0.6390 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 50/1000\n",
      "35/35 [==============================] - 3s 80ms/step - loss: 0.0031 - f1_score: 0.0035 - recall_8: 0.6680 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 51/1000\n",
      "35/35 [==============================] - 3s 81ms/step - loss: 0.0032 - f1_score: 0.0035 - recall_8: 0.6473 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 52/1000\n",
      "35/35 [==============================] - 3s 85ms/step - loss: 0.0033 - f1_score: 0.0035 - recall_8: 0.6390 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 53/1000\n",
      "35/35 [==============================] - 3s 89ms/step - loss: 0.0032 - f1_score: 0.0035 - recall_8: 0.6432 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 54/1000\n",
      "35/35 [==============================] - 3s 83ms/step - loss: 0.0032 - f1_score: 0.0035 - recall_8: 0.6639 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 55/1000\n",
      "35/35 [==============================] - 3s 86ms/step - loss: 0.0034 - f1_score: 0.0035 - recall_8: 0.6515 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 56/1000\n",
      "35/35 [==============================] - 3s 86ms/step - loss: 0.0031 - f1_score: 0.0035 - recall_8: 0.6556 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 57/1000\n",
      "35/35 [==============================] - 3s 92ms/step - loss: 0.0033 - f1_score: 0.0035 - recall_8: 0.6390 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 58/1000\n",
      "35/35 [==============================] - 3s 100ms/step - loss: 0.0031 - f1_score: 0.0035 - recall_8: 0.6598 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 59/1000\n",
      "35/35 [==============================] - 3s 92ms/step - loss: 0.0032 - f1_score: 0.0035 - recall_8: 0.6515 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 60/1000\n",
      "35/35 [==============================] - 3s 85ms/step - loss: 0.0034 - f1_score: 0.0035 - recall_8: 0.6432 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 61/1000\n",
      "35/35 [==============================] - 3s 90ms/step - loss: 0.0034 - f1_score: 0.0035 - recall_8: 0.6598 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 62/1000\n",
      "35/35 [==============================] - 3s 83ms/step - loss: 0.0032 - f1_score: 0.0035 - recall_8: 0.6473 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 63/1000\n",
      "35/35 [==============================] - 3s 84ms/step - loss: 0.0031 - f1_score: 0.0035 - recall_8: 0.6805 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 64/1000\n",
      "35/35 [==============================] - 3s 90ms/step - loss: 0.0034 - f1_score: 0.0035 - recall_8: 0.6266 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 65/1000\n",
      "35/35 [==============================] - 3s 85ms/step - loss: 0.0032 - f1_score: 0.0035 - recall_8: 0.6929 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 66/1000\n",
      "35/35 [==============================] - 3s 93ms/step - loss: 0.0032 - f1_score: 0.0035 - recall_8: 0.6515 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 67/1000\n",
      "35/35 [==============================] - 3s 90ms/step - loss: 0.0033 - f1_score: 0.0035 - recall_8: 0.6432 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 68/1000\n",
      "35/35 [==============================] - 3s 91ms/step - loss: 0.0032 - f1_score: 0.0035 - recall_8: 0.6432 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 69/1000\n",
      "35/35 [==============================] - 4s 105ms/step - loss: 0.0032 - f1_score: 0.0035 - recall_8: 0.6556 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 70/1000\n",
      "35/35 [==============================] - 4s 106ms/step - loss: 0.0032 - f1_score: 0.0035 - recall_8: 0.6432 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 71/1000\n",
      "35/35 [==============================] - 17s 490ms/step - loss: 0.0030 - f1_score: 0.0035 - recall_8: 0.6639 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 72/1000\n",
      "35/35 [==============================] - 16s 453ms/step - loss: 0.0032 - f1_score: 0.0035 - recall_8: 0.6224 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 73/1000\n",
      "35/35 [==============================] - 4s 113ms/step - loss: 0.0033 - f1_score: 0.0035 - recall_8: 0.6888 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 74/1000\n",
      "35/35 [==============================] - 4s 111ms/step - loss: 0.0033 - f1_score: 0.0035 - recall_8: 0.6639 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 75/1000\n",
      "35/35 [==============================] - 3s 102ms/step - loss: 0.0032 - f1_score: 0.0035 - recall_8: 0.6473 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 76/1000\n",
      "35/35 [==============================] - 3s 84ms/step - loss: 0.0033 - f1_score: 0.0035 - recall_8: 0.6556 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 77/1000\n",
      "35/35 [==============================] - 3s 79ms/step - loss: 0.0032 - f1_score: 0.0035 - recall_8: 0.6432 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 78/1000\n",
      "35/35 [==============================] - 3s 80ms/step - loss: 0.0033 - f1_score: 0.0035 - recall_8: 0.6390 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 79/1000\n",
      "35/35 [==============================] - 3s 94ms/step - loss: 0.0030 - f1_score: 0.0035 - recall_8: 0.6888 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 80/1000\n",
      "35/35 [==============================] - 3s 99ms/step - loss: 0.0031 - f1_score: 0.0035 - recall_8: 0.6556 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 81/1000\n",
      "35/35 [==============================] - 3s 90ms/step - loss: 0.0034 - f1_score: 0.0035 - recall_8: 0.6141 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 82/1000\n",
      "35/35 [==============================] - 3s 85ms/step - loss: 0.0032 - f1_score: 0.0035 - recall_8: 0.6805 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 83/1000\n",
      "35/35 [==============================] - 3s 93ms/step - loss: 0.0031 - f1_score: 0.0035 - recall_8: 0.6805 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 84/1000\n",
      "35/35 [==============================] - 3s 92ms/step - loss: 0.0033 - f1_score: 0.0035 - recall_8: 0.6349 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 85/1000\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 0.0033 - f1_score: 0.0035 - recall_8: 0.6639 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 86/1000\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 0.0031 - f1_score: 0.0035 - recall_8: 0.6598 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 87/1000\n",
      "35/35 [==============================] - 3s 84ms/step - loss: 0.0033 - f1_score: 0.0035 - recall_8: 0.6598 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 88/1000\n",
      "35/35 [==============================] - 3s 85ms/step - loss: 0.0032 - f1_score: 0.0035 - recall_8: 0.6763 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 89/1000\n",
      "35/35 [==============================] - 3s 84ms/step - loss: 0.0031 - f1_score: 0.0035 - recall_8: 0.6846 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 90/1000\n",
      "35/35 [==============================] - 3s 96ms/step - loss: 0.0031 - f1_score: 0.0035 - recall_8: 0.6349 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 91/1000\n",
      "35/35 [==============================] - 3s 94ms/step - loss: 0.0031 - f1_score: 0.0035 - recall_8: 0.6888 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 92/1000\n",
      "35/35 [==============================] - 4s 105ms/step - loss: 0.0033 - f1_score: 0.0035 - recall_8: 0.6556 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 93/1000\n",
      "35/35 [==============================] - 4s 108ms/step - loss: 0.0034 - f1_score: 0.0035 - recall_8: 0.6266 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 94/1000\n",
      "35/35 [==============================] - 4s 114ms/step - loss: 0.0032 - f1_score: 0.0035 - recall_8: 0.6846 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 95/1000\n",
      "35/35 [==============================] - 4s 121ms/step - loss: 0.0035 - f1_score: 0.0035 - recall_8: 0.6722 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 96/1000\n",
      "35/35 [==============================] - 4s 127ms/step - loss: 0.0031 - f1_score: 0.0035 - recall_8: 0.6722 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 97/1000\n",
      "35/35 [==============================] - 4s 112ms/step - loss: 0.0033 - f1_score: 0.0035 - recall_8: 0.6390 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 98/1000\n",
      "35/35 [==============================] - 4s 107ms/step - loss: 0.0032 - f1_score: 0.0035 - recall_8: 0.6680 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 99/1000\n",
      "35/35 [==============================] - 4s 103ms/step - loss: 0.0032 - f1_score: 0.0035 - recall_8: 0.6680 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 100/1000\n",
      "35/35 [==============================] - 4s 104ms/step - loss: 0.0031 - f1_score: 0.0035 - recall_8: 0.6763 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 101/1000\n",
      "35/35 [==============================] - 4s 119ms/step - loss: 0.0032 - f1_score: 0.0035 - recall_8: 0.6722 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 102/1000\n",
      "35/35 [==============================] - 4s 115ms/step - loss: 0.0033 - f1_score: 0.0035 - recall_8: 0.6556 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 103/1000\n",
      "35/35 [==============================] - 4s 119ms/step - loss: 0.0032 - f1_score: 0.0035 - recall_8: 0.6473 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 104/1000\n",
      "35/35 [==============================] - 4s 130ms/step - loss: 0.0031 - f1_score: 0.0035 - recall_8: 0.6432 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 105/1000\n",
      "35/35 [==============================] - 4s 122ms/step - loss: 0.0031 - f1_score: 0.0035 - recall_8: 0.6929 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 106/1000\n",
      "35/35 [==============================] - 4s 117ms/step - loss: 0.0031 - f1_score: 0.0035 - recall_8: 0.6432 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 107/1000\n",
      "35/35 [==============================] - 4s 123ms/step - loss: 0.0032 - f1_score: 0.0035 - recall_8: 0.6556 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 108/1000\n",
      "35/35 [==============================] - 4s 125ms/step - loss: 0.0032 - f1_score: 0.0035 - recall_8: 0.6639 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 109/1000\n",
      "35/35 [==============================] - 4s 119ms/step - loss: 0.0033 - f1_score: 0.0035 - recall_8: 0.6432 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 110/1000\n",
      "35/35 [==============================] - 4s 126ms/step - loss: 0.0033 - f1_score: 0.0035 - recall_8: 0.6390 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 111/1000\n",
      "35/35 [==============================] - 5s 147ms/step - loss: 0.0033 - f1_score: 0.0035 - recall_8: 0.6390 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 112/1000\n",
      "35/35 [==============================] - 4s 129ms/step - loss: 0.0034 - f1_score: 0.0035 - recall_8: 0.6432 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 113/1000\n",
      "35/35 [==============================] - 5s 146ms/step - loss: 0.0034 - f1_score: 0.0035 - recall_8: 0.6556 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 114/1000\n",
      "35/35 [==============================] - 4s 119ms/step - loss: 0.0032 - f1_score: 0.0035 - recall_8: 0.6680 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 115/1000\n",
      "35/35 [==============================] - 4s 127ms/step - loss: 0.0033 - f1_score: 0.0035 - recall_8: 0.6680 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 116/1000\n",
      "35/35 [==============================] - 4s 130ms/step - loss: 0.0033 - f1_score: 0.0035 - recall_8: 0.6639 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 117/1000\n",
      "35/35 [==============================] - 4s 118ms/step - loss: 0.0034 - f1_score: 0.0035 - recall_8: 0.6349 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 118/1000\n",
      "35/35 [==============================] - 4s 105ms/step - loss: 0.0034 - f1_score: 0.0035 - recall_8: 0.6349 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 119/1000\n",
      "35/35 [==============================] - 4s 111ms/step - loss: 0.0034 - f1_score: 0.0035 - recall_8: 0.6639 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 120/1000\n",
      "35/35 [==============================] - 4s 112ms/step - loss: 0.0032 - f1_score: 0.0035 - recall_8: 0.6639 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 121/1000\n",
      "35/35 [==============================] - 4s 117ms/step - loss: 0.0031 - f1_score: 0.0035 - recall_8: 0.6266 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 122/1000\n",
      "35/35 [==============================] - 4s 129ms/step - loss: 0.0035 - f1_score: 0.0035 - recall_8: 0.6473 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 123/1000\n",
      "35/35 [==============================] - 4s 116ms/step - loss: 0.0033 - f1_score: 0.0035 - recall_8: 0.6307 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 124/1000\n",
      "35/35 [==============================] - 5s 133ms/step - loss: 0.0032 - f1_score: 0.0035 - recall_8: 0.6639 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 125/1000\n",
      "35/35 [==============================] - 4s 114ms/step - loss: 0.0029 - f1_score: 0.0035 - recall_8: 0.6846 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 126/1000\n",
      "35/35 [==============================] - 4s 114ms/step - loss: 0.0033 - f1_score: 0.0035 - recall_8: 0.6598 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 127/1000\n",
      "35/35 [==============================] - 4s 120ms/step - loss: 0.0032 - f1_score: 0.0035 - recall_8: 0.6556 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 128/1000\n",
      "35/35 [==============================] - 4s 116ms/step - loss: 0.0034 - f1_score: 0.0035 - recall_8: 0.6266 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 129/1000\n",
      "35/35 [==============================] - 4s 120ms/step - loss: 0.0033 - f1_score: 0.0035 - recall_8: 0.6556 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 130/1000\n",
      "35/35 [==============================] - 5s 132ms/step - loss: 0.0031 - f1_score: 0.0035 - recall_8: 0.6805 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 131/1000\n",
      "35/35 [==============================] - 4s 129ms/step - loss: 0.0033 - f1_score: 0.0035 - recall_8: 0.7012 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 132/1000\n",
      "35/35 [==============================] - 4s 125ms/step - loss: 0.0033 - f1_score: 0.0035 - recall_8: 0.6432 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 133/1000\n",
      "35/35 [==============================] - 4s 113ms/step - loss: 0.0031 - f1_score: 0.0035 - recall_8: 0.6680 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 134/1000\n",
      "35/35 [==============================] - 4s 127ms/step - loss: 0.0033 - f1_score: 0.0035 - recall_8: 0.6390 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 135/1000\n",
      "35/35 [==============================] - 4s 119ms/step - loss: 0.0032 - f1_score: 0.0035 - recall_8: 0.6639 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 136/1000\n",
      "35/35 [==============================] - 4s 122ms/step - loss: 0.0033 - f1_score: 0.0035 - recall_8: 0.6763 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 137/1000\n",
      "35/35 [==============================] - 4s 130ms/step - loss: 0.0031 - f1_score: 0.0035 - recall_8: 0.6639 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 138/1000\n",
      "35/35 [==============================] - 5s 149ms/step - loss: 0.0032 - f1_score: 0.0035 - recall_8: 0.6598 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 139/1000\n",
      "35/35 [==============================] - 6s 174ms/step - loss: 0.0031 - f1_score: 0.0035 - recall_8: 0.6888 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 140/1000\n",
      "35/35 [==============================] - 6s 170ms/step - loss: 0.0033 - f1_score: 0.0035 - recall_8: 0.6349 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 141/1000\n",
      "35/35 [==============================] - 5s 159ms/step - loss: 0.0032 - f1_score: 0.0035 - recall_8: 0.6515 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 142/1000\n",
      "35/35 [==============================] - 5s 152ms/step - loss: 0.0031 - f1_score: 0.0035 - recall_8: 0.6224 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 143/1000\n",
      "35/35 [==============================] - 5s 146ms/step - loss: 0.0032 - f1_score: 0.0035 - recall_8: 0.6639 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 144/1000\n",
      "35/35 [==============================] - 5s 143ms/step - loss: 0.0032 - f1_score: 0.0035 - recall_8: 0.6556 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 145/1000\n",
      "35/35 [==============================] - 5s 146ms/step - loss: 0.0033 - f1_score: 0.0035 - recall_8: 0.6515 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 146/1000\n",
      "35/35 [==============================] - 4s 131ms/step - loss: 0.0032 - f1_score: 0.0035 - recall_8: 0.6307 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 147/1000\n",
      "35/35 [==============================] - 5s 142ms/step - loss: 0.0034 - f1_score: 0.0035 - recall_8: 0.6432 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 148/1000\n",
      "35/35 [==============================] - 5s 140ms/step - loss: 0.0031 - f1_score: 0.0035 - recall_8: 0.6846 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 149/1000\n",
      "35/35 [==============================] - 5s 138ms/step - loss: 0.0033 - f1_score: 0.0035 - recall_8: 0.6639 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 150/1000\n",
      "35/35 [==============================] - 5s 137ms/step - loss: 0.0036 - f1_score: 0.0035 - recall_8: 0.6473 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 151/1000\n",
      "35/35 [==============================] - 5s 152ms/step - loss: 0.0031 - f1_score: 0.0035 - recall_8: 0.6473 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6602\n",
      "Epoch 152/1000\n",
      "35/35 [==============================] - 5s 138ms/step - loss: 0.0034 - f1_score: 0.0035 - recall_8: 0.6473 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 153/1000\n",
      "35/35 [==============================] - 5s 149ms/step - loss: 0.0033 - f1_score: 0.0035 - recall_8: 0.6598 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 154/1000\n",
      "35/35 [==============================] - 5s 153ms/step - loss: 0.0033 - f1_score: 0.0035 - recall_8: 0.6556 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 155/1000\n",
      "35/35 [==============================] - 6s 165ms/step - loss: 0.0033 - f1_score: 0.0035 - recall_8: 0.6224 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 156/1000\n",
      "35/35 [==============================] - 6s 167ms/step - loss: 0.0031 - f1_score: 0.0035 - recall_8: 0.6473 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 157/1000\n",
      "35/35 [==============================] - 5s 148ms/step - loss: 0.0033 - f1_score: 0.0035 - recall_8: 0.6515 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 158/1000\n",
      "35/35 [==============================] - 6s 167ms/step - loss: 0.0033 - f1_score: 0.0035 - recall_8: 0.6349 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 159/1000\n",
      "35/35 [==============================] - 5s 155ms/step - loss: 0.0033 - f1_score: 0.0035 - recall_8: 0.6556 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 160/1000\n",
      "35/35 [==============================] - 5s 143ms/step - loss: 0.0030 - f1_score: 0.0035 - recall_8: 0.6722 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 161/1000\n",
      "35/35 [==============================] - 4s 126ms/step - loss: 0.0034 - f1_score: 0.0035 - recall_8: 0.6349 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 162/1000\n",
      "35/35 [==============================] - 4s 128ms/step - loss: 0.0034 - f1_score: 0.0035 - recall_8: 0.6515 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 163/1000\n",
      "35/35 [==============================] - 4s 129ms/step - loss: 0.0031 - f1_score: 0.0035 - recall_8: 0.6722 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 164/1000\n",
      "35/35 [==============================] - 4s 124ms/step - loss: 0.0032 - f1_score: 0.0035 - recall_8: 0.6390 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 165/1000\n",
      "35/35 [==============================] - 4s 129ms/step - loss: 0.0034 - f1_score: 0.0035 - recall_8: 0.6307 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 166/1000\n",
      "35/35 [==============================] - 4s 126ms/step - loss: 0.0032 - f1_score: 0.0035 - recall_8: 0.6763 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 167/1000\n",
      "35/35 [==============================] - 4s 128ms/step - loss: 0.0031 - f1_score: 0.0035 - recall_8: 0.6929 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 168/1000\n",
      "35/35 [==============================] - 4s 129ms/step - loss: 0.0031 - f1_score: 0.0035 - recall_8: 0.6556 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 169/1000\n",
      "35/35 [==============================] - 4s 129ms/step - loss: 0.0034 - f1_score: 0.0035 - recall_8: 0.6266 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 170/1000\n",
      "35/35 [==============================] - 4s 127ms/step - loss: 0.0033 - f1_score: 0.0035 - recall_8: 0.6473 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 171/1000\n",
      "35/35 [==============================] - 5s 143ms/step - loss: 0.0031 - f1_score: 0.0035 - recall_8: 0.6680 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 172/1000\n",
      "35/35 [==============================] - 4s 120ms/step - loss: 0.0032 - f1_score: 0.0035 - recall_8: 0.6971 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 173/1000\n",
      "35/35 [==============================] - 4s 126ms/step - loss: 0.0034 - f1_score: 0.0035 - recall_8: 0.6805 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 174/1000\n",
      "35/35 [==============================] - 4s 127ms/step - loss: 0.0031 - f1_score: 0.0035 - recall_8: 0.6639 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 175/1000\n",
      "35/35 [==============================] - 4s 123ms/step - loss: 0.0031 - f1_score: 0.0035 - recall_8: 0.6639 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 176/1000\n",
      "35/35 [==============================] - 4s 125ms/step - loss: 0.0030 - f1_score: 0.0035 - recall_8: 0.7178 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 177/1000\n",
      "35/35 [==============================] - 4s 120ms/step - loss: 0.0033 - f1_score: 0.0035 - recall_8: 0.6722 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 178/1000\n",
      "35/35 [==============================] - 5s 137ms/step - loss: 0.0032 - f1_score: 0.0035 - recall_8: 0.6473 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 179/1000\n",
      "35/35 [==============================] - 5s 144ms/step - loss: 0.0034 - f1_score: 0.0035 - recall_8: 0.6390 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 180/1000\n",
      "35/35 [==============================] - 5s 143ms/step - loss: 0.0031 - f1_score: 0.0035 - recall_8: 0.6763 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 181/1000\n",
      "35/35 [==============================] - 5s 132ms/step - loss: 0.0032 - f1_score: 0.0035 - recall_8: 0.6722 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 182/1000\n",
      "35/35 [==============================] - 5s 136ms/step - loss: 0.0032 - f1_score: 0.0035 - recall_8: 0.6805 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 183/1000\n",
      "35/35 [==============================] - 5s 138ms/step - loss: 0.0032 - f1_score: 0.0035 - recall_8: 0.6763 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 184/1000\n",
      "35/35 [==============================] - 5s 145ms/step - loss: 0.0032 - f1_score: 0.0035 - recall_8: 0.6598 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 185/1000\n",
      "35/35 [==============================] - 5s 144ms/step - loss: 0.0031 - f1_score: 0.0035 - recall_8: 0.6763 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 186/1000\n",
      "35/35 [==============================] - 5s 145ms/step - loss: 0.0032 - f1_score: 0.0035 - recall_8: 0.6639 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 187/1000\n",
      "35/35 [==============================] - 5s 140ms/step - loss: 0.0033 - f1_score: 0.0035 - recall_8: 0.6390 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 188/1000\n",
      "35/35 [==============================] - 5s 140ms/step - loss: 0.0033 - f1_score: 0.0035 - recall_8: 0.6722 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 189/1000\n",
      "35/35 [==============================] - 5s 142ms/step - loss: 0.0032 - f1_score: 0.0035 - recall_8: 0.6680 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 190/1000\n",
      "35/35 [==============================] - 5s 153ms/step - loss: 0.0031 - f1_score: 0.0035 - recall_8: 0.6598 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 191/1000\n",
      "35/35 [==============================] - 5s 146ms/step - loss: 0.0032 - f1_score: 0.0035 - recall_8: 0.6722 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 192/1000\n",
      "35/35 [==============================] - 5s 146ms/step - loss: 0.0032 - f1_score: 0.0035 - recall_8: 0.6515 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 193/1000\n",
      "35/35 [==============================] - 5s 147ms/step - loss: 0.0031 - f1_score: 0.0035 - recall_8: 0.6598 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 194/1000\n",
      "35/35 [==============================] - 5s 158ms/step - loss: 0.0032 - f1_score: 0.0035 - recall_8: 0.6888 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 195/1000\n",
      "35/35 [==============================] - 5s 146ms/step - loss: 0.0031 - f1_score: 0.0035 - recall_8: 0.6224 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 196/1000\n",
      "35/35 [==============================] - 5s 145ms/step - loss: 0.0033 - f1_score: 0.0035 - recall_8: 0.6680 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 197/1000\n",
      "35/35 [==============================] - 5s 156ms/step - loss: 0.0033 - f1_score: 0.0035 - recall_8: 0.6722 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 198/1000\n",
      "35/35 [==============================] - 5s 146ms/step - loss: 0.0030 - f1_score: 0.0035 - recall_8: 0.6763 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 199/1000\n",
      "35/35 [==============================] - 5s 152ms/step - loss: 0.0030 - f1_score: 0.0035 - recall_8: 0.6473 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 200/1000\n",
      "35/35 [==============================] - 5s 153ms/step - loss: 0.0031 - f1_score: 0.0035 - recall_8: 0.6763 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 201/1000\n",
      "35/35 [==============================] - 5s 153ms/step - loss: 0.0031 - f1_score: 0.0035 - recall_8: 0.6556 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 202/1000\n",
      "35/35 [==============================] - 5s 155ms/step - loss: 0.0034 - f1_score: 0.0035 - recall_8: 0.6390 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 203/1000\n",
      "35/35 [==============================] - 6s 170ms/step - loss: 0.0032 - f1_score: 0.0035 - recall_8: 0.6390 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 204/1000\n",
      "35/35 [==============================] - 5s 141ms/step - loss: 0.0033 - f1_score: 0.0035 - recall_8: 0.6598 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 205/1000\n",
      "35/35 [==============================] - 5s 135ms/step - loss: 0.0031 - f1_score: 0.0035 - recall_8: 0.6390 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 206/1000\n",
      "35/35 [==============================] - 4s 123ms/step - loss: 0.0033 - f1_score: 0.0035 - recall_8: 0.6556 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 207/1000\n",
      "35/35 [==============================] - 4s 125ms/step - loss: 0.0032 - f1_score: 0.0035 - recall_8: 0.6846 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 208/1000\n",
      "35/35 [==============================] - 4s 123ms/step - loss: 0.0030 - f1_score: 0.0035 - recall_8: 0.6473 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 209/1000\n",
      "35/35 [==============================] - 4s 119ms/step - loss: 0.0032 - f1_score: 0.0035 - recall_8: 0.6598 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 210/1000\n",
      "35/35 [==============================] - 4s 119ms/step - loss: 0.0033 - f1_score: 0.0035 - recall_8: 0.6805 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 211/1000\n",
      "35/35 [==============================] - 4s 124ms/step - loss: 0.0031 - f1_score: 0.0035 - recall_8: 0.6473 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 212/1000\n",
      "35/35 [==============================] - 4s 123ms/step - loss: 0.0033 - f1_score: 0.0035 - recall_8: 0.6473 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 213/1000\n",
      "35/35 [==============================] - 4s 129ms/step - loss: 0.0033 - f1_score: 0.0035 - recall_8: 0.6307 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 214/1000\n",
      "35/35 [==============================] - 4s 117ms/step - loss: 0.0032 - f1_score: 0.0035 - recall_8: 0.6473 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 215/1000\n",
      "35/35 [==============================] - 4s 124ms/step - loss: 0.0034 - f1_score: 0.0035 - recall_8: 0.6639 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 216/1000\n",
      "35/35 [==============================] - 4s 121ms/step - loss: 0.0030 - f1_score: 0.0035 - recall_8: 0.6805 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 217/1000\n",
      "35/35 [==============================] - 4s 116ms/step - loss: 0.0032 - f1_score: 0.0035 - recall_8: 0.6680 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 218/1000\n",
      "35/35 [==============================] - 4s 113ms/step - loss: 0.0033 - f1_score: 0.0035 - recall_8: 0.6307 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 219/1000\n",
      "35/35 [==============================] - 4s 128ms/step - loss: 0.0031 - f1_score: 0.0035 - recall_8: 0.6763 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 220/1000\n",
      "35/35 [==============================] - 4s 118ms/step - loss: 0.0033 - f1_score: 0.0035 - recall_8: 0.6183 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 221/1000\n",
      "35/35 [==============================] - 4s 124ms/step - loss: 0.0031 - f1_score: 0.0035 - recall_8: 0.6432 - val_loss: 0.0044 - val_f1_score: 0.0034 - val_recall_8: 0.6699\n",
      "Epoch 221: early stopping\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "    \n",
    "# early stopping 을 위한 validation set 추가\n",
    "pr = Preprocessing(early_stopping=True)\n",
    "x_train, y_train = pr.load_data(\"dataset/early_stopping/train_set.csv\")\n",
    "x_validation, y_validation = pr.load_data(\"dataset/early_stopping/validation_set.csv\")\n",
    "x_test, y_test = pr.load_data(\"dataset/early_stopping/test_set.csv\")\n",
    "\n",
    "# Normalization 방법 바꿈. - standard scaler 이용 - data imvalance에 좋다고함. 원리는 모름 \n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_validation = scaler.fit_transform(x_validation)\n",
    "x_test = scaler.fit_transform(x_test)\n",
    "\n",
    "# Model Build\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(29, activation = 'relu', kernel_initializer='he_uniform'), # 입력층 에 kaiming weight initialization 추가 \n",
    "    tf.keras.layers.Dropout(0.2),                                                    # dropout 으로 랜덤으로 신경망 끄기  \n",
    "    tf.keras.layers.BatchNormalization(synchronized=True),                            # batch normalization으로 중간중간 정규화 \n",
    "    tf.keras.layers.Dense(15, activation = 'relu', kernel_initializer='he_uniform'), # 은닉층 (필요한가 ??)에 kaiming weight initialization 추가 \n",
    "    tf.keras.layers.Dropout(0.2),                                                    # dropout 으로 랜덤으로 신경망 끄기  \n",
    "    tf.keras.layers.BatchNormalization(synchronized=True),                            # batch normalization으로 중간중간 정규화\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')                                   # 출력층\n",
    "])\n",
    "\n",
    "# Hyper-parameter tunning (optmizer, cost function)\n",
    "learning_rate = 0.008\n",
    "batch_size = 4096   # 2^12, 그래픽카드가 없어 gpu 활용은 힘듬. 대신 batch size 세팅을 했더니 속도가 많이 개선됨\n",
    "epoch= 1000\n",
    "\n",
    "# learning rate decay 관련 \n",
    "warmup_epoch = int(epoch * 0.1)\n",
    "init_lr = 0.1\n",
    "min_lr = 1e-6\n",
    "power = 1.\n",
    "\n",
    "# early stopping patience 개수 지정\n",
    "patience = 200\n",
    "\n",
    "\n",
    "# ndarray -> dataset 변환 후 batch size세팅\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(batch_size) # buffer size?? \n",
    "\n",
    "# learning rate decay with warmup : 0.1~ 1e-6 까지 learning rate 조정되도록 세팅 \n",
    "lr_scheduler = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "    initial_learning_rate = init_lr,\n",
    "    decay_steps = epoch - warmup_epoch,\n",
    "    end_learning_rate = min_lr,\n",
    "    power = power\n",
    ")\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr_scheduler),\n",
    "              loss = 'binary_crossentropy',\n",
    "              metrics=[ tf.keras.metrics.F1Score(), tf.keras.metrics.Recall()])  # 비대칭 데이터 이고 이상거래 탐지가 핵심이므로  recall 수치 필요 \n",
    "\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=patience) # with 40 patience\n",
    "\n",
    "# Training\n",
    "try:\n",
    "    history = model.fit(dataset, validation_data=(x_validation, y_validation),  epochs=epoch,  callbacks=[es])\n",
    "except Exception as e: \n",
    "    print(e)\n",
    "    raise e\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluation\n",
    "train_loss, train_f1, train_recall = model.evaluate(x_train, y_train, verbose=0)\n",
    "#validation_loss, validation_f1, validation_recall = model.evaluate(x_validation, y_validation, verbose=0)\n",
    "test_loss, test_f1, test_recall = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print(f\"Train f1 score: {train_f1}, Test f1 score: {test_f1}\")\n",
    "print(f\"Train recall: {train_recall}, Test recall: {test_recall}\")\n",
    "\n",
    "# Visualization\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='validation')\n",
    "plt.xlabel(\"The number of learning\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Evaluation\n",
    "loss, f1_score, recall = model.evaluate(x_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "성능 안좋음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 개선버전2: Regularization\n",
    "배운 것  모두 다 넣어서 테스트 \n",
    "- dropout\n",
    "- standard scaler\n",
    "- batch normalization\n",
    "- weight initialization \n",
    "- l2 regularizer 추가 \n",
    "- learning rate decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "49/49 [==============================] - 4s 15ms/step - loss: 0.1585 - f1_score: 0.0034 - recall_10: 0.4855\n",
      "Epoch 2/100\n",
      "49/49 [==============================] - 1s 16ms/step - loss: 0.0233 - f1_score: 0.0034 - recall_10: 0.4506\n",
      "Epoch 3/100\n",
      "49/49 [==============================] - 1s 16ms/step - loss: 0.0154 - f1_score: 0.0034 - recall_10: 0.4797\n",
      "Epoch 4/100\n",
      "49/49 [==============================] - 1s 16ms/step - loss: 0.0152 - f1_score: 0.0034 - recall_10: 0.5145\n",
      "Epoch 5/100\n",
      "49/49 [==============================] - 1s 14ms/step - loss: 0.0153 - f1_score: 0.0034 - recall_10: 0.5320\n",
      "Epoch 6/100\n",
      "49/49 [==============================] - 1s 15ms/step - loss: 0.0156 - f1_score: 0.0034 - recall_10: 0.5145\n",
      "Epoch 7/100\n",
      "49/49 [==============================] - 1s 13ms/step - loss: 0.0153 - f1_score: 0.0034 - recall_10: 0.4971\n",
      "Epoch 8/100\n",
      "49/49 [==============================] - 1s 11ms/step - loss: 0.0151 - f1_score: 0.0034 - recall_10: 0.5233\n",
      "Epoch 9/100\n",
      "49/49 [==============================] - 1s 12ms/step - loss: 0.0153 - f1_score: 0.0034 - recall_10: 0.5145\n",
      "Epoch 10/100\n",
      "49/49 [==============================] - 1s 12ms/step - loss: 0.0154 - f1_score: 0.0034 - recall_10: 0.5494\n",
      "Epoch 11/100\n",
      "49/49 [==============================] - 1s 11ms/step - loss: 0.0151 - f1_score: 0.0034 - recall_10: 0.5058\n",
      "Epoch 12/100\n",
      "49/49 [==============================] - 1s 12ms/step - loss: 0.0151 - f1_score: 0.0034 - recall_10: 0.5378\n",
      "Epoch 13/100\n",
      "49/49 [==============================] - 1s 11ms/step - loss: 0.0151 - f1_score: 0.0034 - recall_10: 0.5058\n",
      "Epoch 14/100\n",
      "49/49 [==============================] - 1s 11ms/step - loss: 0.0155 - f1_score: 0.0034 - recall_10: 0.5174\n",
      "Epoch 15/100\n",
      "49/49 [==============================] - 1s 16ms/step - loss: 0.0148 - f1_score: 0.0034 - recall_10: 0.5320\n",
      "Epoch 16/100\n",
      "49/49 [==============================] - 1s 21ms/step - loss: 0.0155 - f1_score: 0.0034 - recall_10: 0.4884\n",
      "Epoch 17/100\n",
      "49/49 [==============================] - 1s 28ms/step - loss: 0.0152 - f1_score: 0.0034 - recall_10: 0.5000\n",
      "Epoch 18/100\n",
      "49/49 [==============================] - 1s 23ms/step - loss: 0.0151 - f1_score: 0.0034 - recall_10: 0.5174\n",
      "Epoch 19/100\n",
      "49/49 [==============================] - 1s 26ms/step - loss: 0.0153 - f1_score: 0.0034 - recall_10: 0.5145\n",
      "Epoch 20/100\n",
      "49/49 [==============================] - 1s 25ms/step - loss: 0.0154 - f1_score: 0.0034 - recall_10: 0.5262\n",
      "Epoch 21/100\n",
      "49/49 [==============================] - 1s 22ms/step - loss: 0.0150 - f1_score: 0.0034 - recall_10: 0.5087\n",
      "Epoch 22/100\n",
      "49/49 [==============================] - 1s 18ms/step - loss: 0.0151 - f1_score: 0.0034 - recall_10: 0.5087\n",
      "Epoch 23/100\n",
      "49/49 [==============================] - 1s 10ms/step - loss: 0.0155 - f1_score: 0.0034 - recall_10: 0.4767\n",
      "Epoch 24/100\n",
      "49/49 [==============================] - 0s 10ms/step - loss: 0.0150 - f1_score: 0.0034 - recall_10: 0.5378\n",
      "Epoch 25/100\n",
      "49/49 [==============================] - 0s 10ms/step - loss: 0.0148 - f1_score: 0.0034 - recall_10: 0.5203\n",
      "Epoch 26/100\n",
      "49/49 [==============================] - 1s 10ms/step - loss: 0.0156 - f1_score: 0.0034 - recall_10: 0.4884\n",
      "Epoch 27/100\n",
      "49/49 [==============================] - 1s 11ms/step - loss: 0.0151 - f1_score: 0.0034 - recall_10: 0.4942\n",
      "Epoch 28/100\n",
      "49/49 [==============================] - 1s 10ms/step - loss: 0.0151 - f1_score: 0.0034 - recall_10: 0.5407\n",
      "Epoch 29/100\n",
      "49/49 [==============================] - 1s 10ms/step - loss: 0.0153 - f1_score: 0.0034 - recall_10: 0.5000\n",
      "Epoch 30/100\n",
      "49/49 [==============================] - 1s 11ms/step - loss: 0.0153 - f1_score: 0.0034 - recall_10: 0.5233\n",
      "Epoch 31/100\n",
      "49/49 [==============================] - 0s 10ms/step - loss: 0.0150 - f1_score: 0.0034 - recall_10: 0.5320\n",
      "Epoch 32/100\n",
      "49/49 [==============================] - 0s 10ms/step - loss: 0.0150 - f1_score: 0.0034 - recall_10: 0.5174\n",
      "Epoch 33/100\n",
      "49/49 [==============================] - 0s 10ms/step - loss: 0.0148 - f1_score: 0.0034 - recall_10: 0.5087\n",
      "Epoch 34/100\n",
      "49/49 [==============================] - 1s 10ms/step - loss: 0.0156 - f1_score: 0.0034 - recall_10: 0.5029\n",
      "Epoch 35/100\n",
      "49/49 [==============================] - 1s 10ms/step - loss: 0.0150 - f1_score: 0.0034 - recall_10: 0.5000\n",
      "Epoch 36/100\n",
      "49/49 [==============================] - 0s 10ms/step - loss: 0.0150 - f1_score: 0.0034 - recall_10: 0.5174\n",
      "Epoch 37/100\n",
      "49/49 [==============================] - 1s 15ms/step - loss: 0.0147 - f1_score: 0.0034 - recall_10: 0.5233\n",
      "Epoch 38/100\n",
      "49/49 [==============================] - 1s 13ms/step - loss: 0.0150 - f1_score: 0.0034 - recall_10: 0.5029\n",
      "Epoch 39/100\n",
      "49/49 [==============================] - 1s 16ms/step - loss: 0.0154 - f1_score: 0.0034 - recall_10: 0.5174\n",
      "Epoch 40/100\n",
      "49/49 [==============================] - 1s 15ms/step - loss: 0.0150 - f1_score: 0.0034 - recall_10: 0.5087\n",
      "Epoch 41/100\n",
      "49/49 [==============================] - 1s 13ms/step - loss: 0.0153 - f1_score: 0.0034 - recall_10: 0.5378\n",
      "Epoch 42/100\n",
      "49/49 [==============================] - 1s 11ms/step - loss: 0.0148 - f1_score: 0.0034 - recall_10: 0.5320\n",
      "Epoch 43/100\n",
      "49/49 [==============================] - 1s 10ms/step - loss: 0.0148 - f1_score: 0.0034 - recall_10: 0.5436\n",
      "Epoch 44/100\n",
      "49/49 [==============================] - 0s 10ms/step - loss: 0.0153 - f1_score: 0.0034 - recall_10: 0.4942\n",
      "Epoch 45/100\n",
      "49/49 [==============================] - 0s 10ms/step - loss: 0.0150 - f1_score: 0.0034 - recall_10: 0.5203\n",
      "Epoch 46/100\n",
      "49/49 [==============================] - 1s 11ms/step - loss: 0.0155 - f1_score: 0.0034 - recall_10: 0.5058\n",
      "Epoch 47/100\n",
      "49/49 [==============================] - 1s 14ms/step - loss: 0.0146 - f1_score: 0.0034 - recall_10: 0.5233\n",
      "Epoch 48/100\n",
      "49/49 [==============================] - 1s 13ms/step - loss: 0.0151 - f1_score: 0.0034 - recall_10: 0.5116\n",
      "Epoch 49/100\n",
      "49/49 [==============================] - 1s 13ms/step - loss: 0.0150 - f1_score: 0.0034 - recall_10: 0.4942\n",
      "Epoch 50/100\n",
      "49/49 [==============================] - 1s 13ms/step - loss: 0.0149 - f1_score: 0.0034 - recall_10: 0.5203\n",
      "Epoch 51/100\n",
      "49/49 [==============================] - 1s 10ms/step - loss: 0.0153 - f1_score: 0.0034 - recall_10: 0.5058\n",
      "Epoch 52/100\n",
      "49/49 [==============================] - 1s 15ms/step - loss: 0.0152 - f1_score: 0.0034 - recall_10: 0.5087\n",
      "Epoch 53/100\n",
      "49/49 [==============================] - 1s 12ms/step - loss: 0.0152 - f1_score: 0.0034 - recall_10: 0.4884\n",
      "Epoch 54/100\n",
      "49/49 [==============================] - 1s 11ms/step - loss: 0.0150 - f1_score: 0.0034 - recall_10: 0.5233\n",
      "Epoch 55/100\n",
      "49/49 [==============================] - 0s 10ms/step - loss: 0.0150 - f1_score: 0.0034 - recall_10: 0.5378\n",
      "Epoch 56/100\n",
      "49/49 [==============================] - 0s 10ms/step - loss: 0.0147 - f1_score: 0.0034 - recall_10: 0.5291\n",
      "Epoch 57/100\n",
      "49/49 [==============================] - 0s 10ms/step - loss: 0.0147 - f1_score: 0.0034 - recall_10: 0.5233\n",
      "Epoch 58/100\n",
      "49/49 [==============================] - 1s 14ms/step - loss: 0.0155 - f1_score: 0.0034 - recall_10: 0.5058\n",
      "Epoch 59/100\n",
      "49/49 [==============================] - 1s 12ms/step - loss: 0.0152 - f1_score: 0.0034 - recall_10: 0.4738\n",
      "Epoch 60/100\n",
      "49/49 [==============================] - 1s 13ms/step - loss: 0.0152 - f1_score: 0.0034 - recall_10: 0.5407\n",
      "Epoch 61/100\n",
      "49/49 [==============================] - 1s 12ms/step - loss: 0.0148 - f1_score: 0.0034 - recall_10: 0.5233\n",
      "Epoch 62/100\n",
      "49/49 [==============================] - 1s 12ms/step - loss: 0.0149 - f1_score: 0.0034 - recall_10: 0.5087\n",
      "Epoch 63/100\n",
      "49/49 [==============================] - 1s 14ms/step - loss: 0.0151 - f1_score: 0.0034 - recall_10: 0.5058\n",
      "Epoch 64/100\n",
      "49/49 [==============================] - 1s 12ms/step - loss: 0.0150 - f1_score: 0.0034 - recall_10: 0.5203\n",
      "Epoch 65/100\n",
      "49/49 [==============================] - 1s 17ms/step - loss: 0.0147 - f1_score: 0.0034 - recall_10: 0.5029\n",
      "Epoch 66/100\n",
      "49/49 [==============================] - 1s 14ms/step - loss: 0.0153 - f1_score: 0.0034 - recall_10: 0.4942\n",
      "Epoch 67/100\n",
      "49/49 [==============================] - 1s 12ms/step - loss: 0.0150 - f1_score: 0.0034 - recall_10: 0.5029\n",
      "Epoch 68/100\n",
      "49/49 [==============================] - 1s 13ms/step - loss: 0.0155 - f1_score: 0.0034 - recall_10: 0.4884\n",
      "Epoch 69/100\n",
      "49/49 [==============================] - 1s 15ms/step - loss: 0.0152 - f1_score: 0.0034 - recall_10: 0.5262\n",
      "Epoch 70/100\n",
      "49/49 [==============================] - 1s 12ms/step - loss: 0.0148 - f1_score: 0.0034 - recall_10: 0.4971\n",
      "Epoch 71/100\n",
      "49/49 [==============================] - 1s 12ms/step - loss: 0.0149 - f1_score: 0.0034 - recall_10: 0.5145\n",
      "Epoch 72/100\n",
      "49/49 [==============================] - 1s 12ms/step - loss: 0.0149 - f1_score: 0.0034 - recall_10: 0.4942\n",
      "Epoch 73/100\n",
      "49/49 [==============================] - 1s 11ms/step - loss: 0.0146 - f1_score: 0.0034 - recall_10: 0.5087\n",
      "Epoch 74/100\n",
      "49/49 [==============================] - 1s 13ms/step - loss: 0.0150 - f1_score: 0.0034 - recall_10: 0.4913\n",
      "Epoch 75/100\n",
      "49/49 [==============================] - 1s 14ms/step - loss: 0.0152 - f1_score: 0.0034 - recall_10: 0.5203\n",
      "Epoch 76/100\n",
      "49/49 [==============================] - 1s 14ms/step - loss: 0.0149 - f1_score: 0.0034 - recall_10: 0.5087\n",
      "Epoch 77/100\n",
      "49/49 [==============================] - 1s 13ms/step - loss: 0.0146 - f1_score: 0.0034 - recall_10: 0.5407\n",
      "Epoch 78/100\n",
      "49/49 [==============================] - 1s 13ms/step - loss: 0.0149 - f1_score: 0.0034 - recall_10: 0.4971\n",
      "Epoch 79/100\n",
      "49/49 [==============================] - 1s 12ms/step - loss: 0.0149 - f1_score: 0.0034 - recall_10: 0.5058\n",
      "Epoch 80/100\n",
      "49/49 [==============================] - 1s 16ms/step - loss: 0.0142 - f1_score: 0.0034 - recall_10: 0.5407\n",
      "Epoch 81/100\n",
      "49/49 [==============================] - 1s 15ms/step - loss: 0.0149 - f1_score: 0.0034 - recall_10: 0.5058\n",
      "Epoch 82/100\n",
      "49/49 [==============================] - 1s 13ms/step - loss: 0.0150 - f1_score: 0.0034 - recall_10: 0.4855\n",
      "Epoch 83/100\n",
      "49/49 [==============================] - 1s 12ms/step - loss: 0.0145 - f1_score: 0.0034 - recall_10: 0.5494\n",
      "Epoch 84/100\n",
      "49/49 [==============================] - 1s 12ms/step - loss: 0.0147 - f1_score: 0.0034 - recall_10: 0.5291\n",
      "Epoch 85/100\n",
      "49/49 [==============================] - 1s 12ms/step - loss: 0.0143 - f1_score: 0.0034 - recall_10: 0.5262\n",
      "Epoch 86/100\n",
      "49/49 [==============================] - 1s 13ms/step - loss: 0.0148 - f1_score: 0.0034 - recall_10: 0.5320\n",
      "Epoch 87/100\n",
      "49/49 [==============================] - 1s 15ms/step - loss: 0.0146 - f1_score: 0.0034 - recall_10: 0.5203\n",
      "Epoch 88/100\n",
      "49/49 [==============================] - 1s 15ms/step - loss: 0.0146 - f1_score: 0.0034 - recall_10: 0.5145\n",
      "Epoch 89/100\n",
      "49/49 [==============================] - 1s 13ms/step - loss: 0.0144 - f1_score: 0.0034 - recall_10: 0.5203\n",
      "Epoch 90/100\n",
      "49/49 [==============================] - 1s 16ms/step - loss: 0.0150 - f1_score: 0.0034 - recall_10: 0.5116\n",
      "Epoch 91/100\n",
      "49/49 [==============================] - 1s 15ms/step - loss: 0.0143 - f1_score: 0.0034 - recall_10: 0.5174\n",
      "Epoch 92/100\n",
      "49/49 [==============================] - 1s 12ms/step - loss: 0.0151 - f1_score: 0.0034 - recall_10: 0.5029\n",
      "Epoch 93/100\n",
      "49/49 [==============================] - 1s 15ms/step - loss: 0.0148 - f1_score: 0.0034 - recall_10: 0.4826\n",
      "Epoch 94/100\n",
      "49/49 [==============================] - 1s 16ms/step - loss: 0.0142 - f1_score: 0.0034 - recall_10: 0.5320\n",
      "Epoch 95/100\n",
      "49/49 [==============================] - 1s 13ms/step - loss: 0.0146 - f1_score: 0.0034 - recall_10: 0.5378\n",
      "Epoch 96/100\n",
      "49/49 [==============================] - 1s 13ms/step - loss: 0.0140 - f1_score: 0.0034 - recall_10: 0.5494\n",
      "Epoch 97/100\n",
      "49/49 [==============================] - 1s 18ms/step - loss: 0.0150 - f1_score: 0.0034 - recall_10: 0.4942\n",
      "Epoch 98/100\n",
      "49/49 [==============================] - 1s 12ms/step - loss: 0.0143 - f1_score: 0.0034 - recall_10: 0.5320\n",
      "Epoch 99/100\n",
      "49/49 [==============================] - 1s 13ms/step - loss: 0.0146 - f1_score: 0.0034 - recall_10: 0.5145\n",
      "Epoch 100/100\n",
      "49/49 [==============================] - 1s 11ms/step - loss: 0.0145 - f1_score: 0.0034 - recall_10: 0.5174\n"
     ]
    }
   ],
   "source": [
    "pr = Preprocessing()\n",
    "x_train, y_train = pr.load_data(\"dataset/train_set.csv\")\n",
    "x_test, y_test = pr.load_data(\"dataset/test_set.csv\")\n",
    "\n",
    "# Normalization 방법 바꿈. - standard scaler 이용 - data imvalance에 좋다고함. 원리는 모름\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_validation = scaler.fit_transform(x_validation)\n",
    "x_test = scaler.fit_transform(x_test)\n",
    "\n",
    "# Model Build\n",
    "# regularizer 추가\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(\n",
    "        29,\n",
    "        activation='relu',\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "        kernel_initializer='he_uniform'),\n",
    "    # 입력층 에 kaiming weight initialization 추가\n",
    "    # dropout 으로 랜덤으로 신경망 끄기\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    # batch normalization으로 중간중간 정규화\n",
    "    tf.keras.layers.BatchNormalization(synchronized=True),\n",
    "    tf.keras.layers.Dense(\n",
    "        15,\n",
    "        activation='relu',\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "        kernel_initializer='he_uniform'),\n",
    "    # 은닉층 (필요한가 ??)에 kaiming weight initialization 추가\n",
    "    # dropout 으로 랜덤으로 신경망 끄기\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    # batch normalization으로 중간중간 정규화\n",
    "    tf.keras.layers.BatchNormalization(synchronized=True),\n",
    "    # 출력층\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Hyper-parameter tunning (optmizer, cost function)\n",
    "learning_rate = 0.008\n",
    "batch_size = 4096   # 2^12, 그래픽카드가 없어 gpu 활용은 힘듬. 대신 batch size 세팅을 했더니 속도가 많이 개선됨\n",
    "epoch = 100\n",
    "\n",
    "# learning rate decay 관련\n",
    "warmup_epoch = int(epoch * 0.1)\n",
    "init_lr = 0.1\n",
    "min_lr = 1e-6\n",
    "power = 1.\n",
    "\n",
    "# ndarray -> dataset 변환 후 batch size세팅\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(batch_size)  # buffer size??\n",
    "\n",
    "# learning rate decay with warmup : 0.1~ 1e-6 까지 learning rate 조정되도록 세팅\n",
    "lr_scheduler = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "    initial_learning_rate=init_lr,\n",
    "    decay_steps=epoch - warmup_epoch,\n",
    "    end_learning_rate=min_lr,\n",
    "    power=power\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(lr_scheduler),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=[\n",
    "        tf.keras.metrics.F1Score(),\n",
    "         tf.keras.metrics.Recall()])  # 비대칭 데이터 이고 이상거래 탐지가 핵심이므로  recall 수치 필요\n",
    "\n",
    "# Training\n",
    "try:\n",
    "    history = model.fit(dataset, epochs=epoch)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    raise e\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2671/2671 - 3s - loss: 0.0126 - f1_score: 0.0035 - recall_10: 0.6622 - 3s/epoch - 1ms/step\n",
      "f1_score: [0.00345835], recall: 0.662162184715271\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Visualization\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.xlabel(\"The number of learning\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Evaluation\n",
    "loss, f1_score, recall = model.evaluate(x_test, y_test, verbose=2)\n",
    "print(f\"f1_score: {f1_score}, recall: {recall}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "더 안좋음 ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
